{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import scipy\n",
    "from scipy import stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# An example of a class\n",
    "class Network:\n",
    "    def __init__(self, Topo, Train, Test, MaxTime, MinPer):\n",
    "\n",
    "        self.Top = Topo  # NN topology [input, hidden, output]\n",
    "        self.Max = MaxTime  # max epocs\n",
    "        self.TrainData = Train\n",
    "        self.TestData = Test\n",
    "        self.NumSamples = Train.shape[0]\n",
    "\n",
    "        self.lrate = 0  # will be updated later with BP call\n",
    "\n",
    "        self.momenRate = 0\n",
    "\n",
    "        self.minPerf = MinPer\n",
    "        # initialize weights ( W1 W2 ) and bias ( b1 b2 ) of the network\n",
    "        np.random.seed()\n",
    "        self.W1 = np.zeros((self.Top[0], self.Top[1])  )\n",
    "        self.B1 = np.zeros(self.Top[1])    # bias first layer\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestW1 = self.W1\n",
    "        self.W2 = np.zeros((self.Top[1], self.Top[2]) )\n",
    "        self.B2 = np.zeros(self.Top[2])    # bias second layer\n",
    "        self.BestB2 = self.B2\n",
    "        self.BestW2 = self.W2\n",
    "        self.hidout = np.zeros((self.Top[1]))  # output of first hidden layer\n",
    "        self.out = np.zeros((self.Top[2]))  # output last layer\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def printNet(self):\n",
    "        print self.Top\n",
    "        print self.W1\n",
    "\n",
    "    def sampleEr(self, actualout):\n",
    "        error = np.subtract(self.out, actualout)\n",
    "        sqerror = np.sum(np.square(error)) / self.Top[2]\n",
    "        return sqerror\n",
    "\n",
    "    def ForwardPass(self, X):\n",
    "        z1 = X.dot(self.W1) - self.B1\n",
    "        self.hidout = self.sigmoid(z1)  # output of first hidden layer#\n",
    "        z2 = self.hidout.dot(self.W2) #- self.B2\n",
    "        self.out = self.sigmoid(z2)  # output second hidden layer\n",
    "\n",
    "    def BackwardPass(self, Input, desired):\n",
    "\n",
    "        out_delta = (desired - self.out) * (self.out * (1 - self.out))\n",
    "        hid_delta = np.zeros(self.Top[2])\n",
    "        hid_delta = out_delta.dot(self.W2.T) * (self.hidout * (1 - self.hidout))\n",
    "\n",
    "    # update weights and bias\n",
    "        layer = 1  # hidden to output\n",
    "        for x in xrange(0, self.Top[layer]):\n",
    "            for y in xrange(0, self.Top[layer + 1]):\n",
    "                self.W2[x, y] += self.lrate * out_delta[y] * self.hidout[x]\n",
    "        for y in xrange(0, self.Top[layer + 1]):\n",
    "            self.B2[y] += -1 * self.lrate * out_delta[y]\n",
    "\n",
    "        layer = 0  # Input to Hidden\n",
    "\n",
    "        for x in xrange(0, self.Top[layer]):\n",
    "            for y in xrange(0, self.Top[layer + 1]):\n",
    "                self.W1[x, y] += self.lrate * hid_delta[y] * Input[x]\n",
    "        for y in xrange(0, self.Top[layer + 1]):\n",
    "            self.B1[y] += -1 * self.lrate * hid_delta[y]\n",
    "\n",
    "\n",
    "    def saveKnowledge(self):\n",
    "        self.BestW1 = self.W1\n",
    "        self.BestW2 = self.W2\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestB2 = self.B2\n",
    "\n",
    "\n",
    "    def decode(self, w):\n",
    "        w_layer1size = self.Top[0] * self.Top[1]\n",
    "        w_layer2size = self.Top[1] * self.Top[2]\n",
    "\n",
    "        w_layer1 = w[0:w_layer1size]\n",
    "        self.W1 = np.reshape(w_layer1, (self.Top[0], self.Top[1]))\n",
    "\n",
    "        w_layer2 = w[w_layer1size:w_layer1size + w_layer2size]\n",
    "        self.W2 = np.reshape(w_layer2, (self.Top[1], self.Top[2]))\n",
    "        self.B1 = w[w_layer1size + w_layer2size:w_layer1size + w_layer2size + self.Top[1]]\n",
    "        #self.B2 = w[w_layer1size + w_layer2size + self.Top[1]:w_layer1size + w_layer2size + self.Top[1] + self.Top[2]]\n",
    "\n",
    "        self.total_weightsbias = w_layer1size + w_layer2size + self.Top[1] + self.Top[2]\n",
    "\n",
    "    def decode_ESPencoding(self, w):\n",
    "        layer = 0\n",
    "        gene = 0\n",
    "\n",
    "\n",
    "\n",
    "        for neu in xrange(0, self.Top[1]): #every input neuron\n",
    "            for row in xrange(0, self.Top[layer]): # for input to each hidden neuron weights (row)\n",
    "                self.W1[row][neu] = w[gene]\n",
    "                gene = gene+1\n",
    "\n",
    "            self.B1[neu] = w[gene]\n",
    "            gene = gene + 1\n",
    "\n",
    "\n",
    "\n",
    "            for row in xrange(0, self.Top[2]): #for each hidden neuron to output weight\n",
    "                self.W2[neu][row] = w[gene]\n",
    "                gene = gene+1\n",
    "\n",
    "\n",
    "\n",
    "        #self.B2[0] = w[gene] # for bias in second layer (assumes one output - change if more than one)\n",
    "\n",
    "\n",
    "    def net_size(self, netw):\n",
    "        return ((netw[0] * netw[1]) + (netw[1] * netw[2]) + netw[1] ) #+ netw[2]\n",
    "\n",
    "\n",
    "\n",
    "    def decode_MTNencodingX(self, w, mtopo, subtasks):\n",
    "\n",
    "\n",
    "        position = 0\n",
    "\n",
    "        Top1 = mtopo[0] #subtask m that corresponds to the construction of input features, Xm\n",
    "\n",
    "\n",
    "        for neu in xrange(0, Top1[1]): \n",
    "            for row in xrange(0, Top1[0]): \n",
    "                self.W1[row, neu] = w[position]\n",
    "                #print neu, row, position, '    -----  a '\n",
    "                position = position + 1 #for each subtask m, the k+1th position in the chain is sampled by proposing new values of distribution of w and b; if proposed values accepted -> it is transferred to the parameter vector of sutask knowledge\n",
    "            self.B1[neu] = w[position]\n",
    "            #print neu,   position, '    -----  b '\n",
    "            position = position + 1\n",
    "\n",
    "        for neu in xrange(0, Top1[2]):\n",
    "            for row in xrange(0, Top1[1]):\n",
    "                self.W2[row, neu] = w[position]\n",
    "                #print neu, row, position, '    -----  c '\n",
    "                position = position + 1\n",
    "\n",
    "\n",
    "        if subtasks >=1:\n",
    "\n",
    "\n",
    "            for step  in xrange(1, subtasks+1   ):\n",
    "\n",
    "                TopPrev = mtopo[step-1] #previous subtask  \n",
    "                TopG = mtopo[step] #subtask m \n",
    "                Hid = TopPrev[1]\n",
    "                Inp = TopPrev[0]\n",
    "\n",
    "\n",
    "                layer = 0\n",
    "\n",
    "                for neu in xrange(Hid , TopG[layer + 1]      ) :\n",
    "                    for row in xrange(0, TopG[layer]   ):\n",
    "                        #print neu, row, position, '    -----  A '\n",
    "\n",
    "                        self.W1[row, neu] = w[position]\n",
    "                        position = position + 1\n",
    "\n",
    "                    self.B1[neu] = w[position]\n",
    "                    #print neu,   position, '    -----  B '\n",
    "                    position = position + 1\n",
    "\n",
    "                diff = (TopG[layer + 1] - TopPrev[layer + 1]) # just the diff in number of hidden neurons between subtasks\n",
    "\n",
    "                for neu in xrange(0, TopG[layer + 1]- diff):  # %\n",
    "                    for row in xrange(Inp , TopG[layer]):\n",
    "                        #print neu, row, position, '    -----  C '\n",
    "                        self.W1[row, neu] = w[position]\n",
    "                        position = position + 1\n",
    "\n",
    "                layer = 1\n",
    "\n",
    "                for neu in xrange(0, TopG[layer + 1]):  # %\n",
    "                    for row in xrange(Hid , TopG[layer]):\n",
    "                        #print neu, row, position, '    -----  D '\n",
    "                        self.W2[row, neu] = w[position]\n",
    "                        position = position + 1\n",
    "\n",
    "                #print w\n",
    "                #print self.W1\n",
    "                #print self.B1\n",
    "                #print self.W2\n",
    "\n",
    "    def encode(self):\n",
    "        w1 = self.W1.ravel()\n",
    "        w2 = self.W2.ravel()\n",
    "        w = np.concatenate([w1, w2, self.B1, self.B2])\n",
    "        return w\n",
    "\n",
    "    def evaluate_proposal(self,   w , mtopo, subtasks):  # BP with SGD (Stocastic BP)\n",
    "\n",
    "        #self.decode(w)  # method to decode w into W1, W2, B1, B2.\n",
    "        self.decode_MTNencodingX(w, mtopo, subtasks)\n",
    "\n",
    "        size = self.TrainData.shape[0]\n",
    "        Input = np.zeros((1, self.Top[0]))  # temp hold input\n",
    "        fx = np.zeros(size)\n",
    "\n",
    "\n",
    "\n",
    "        for pat in xrange(0, size):\n",
    "            Input[:] =  self.TrainData[pat, 0:self.Top[0]]\n",
    "            self.ForwardPass(Input)\n",
    "            fx[pat] = self.out\n",
    "        return fx\n",
    "\n",
    "    def test_proposal(self,  w, mtopo, subtasks):  # BP with SGD (Stocastic BP)\n",
    "\n",
    "        #self.decode(w)  # method to decode w into W1, W2, B1, B2.\n",
    "        self.decode_MTNencodingX(w, mtopo, subtasks)\n",
    "\n",
    "        size = self.TestData.shape[0]\n",
    "        Input = np.zeros((1, self.Top[0]))  # temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2]))\n",
    "        fx = np.zeros(size)\n",
    "\n",
    "        sse = 0\n",
    "\n",
    "        for pat in xrange(0, size):\n",
    "            Input[:] = self.TestData[pat, 0:self.Top[0]]\n",
    "            Desired[:] = self.TestData[pat, self.Top[0]:]\n",
    "            self.ForwardPass(Input)\n",
    "            fx[pat] = self.out\n",
    "            sse = sse + self.sampleEr(Desired)\n",
    "\n",
    "        rmse = np.sqrt(sse / size)\n",
    "\n",
    "\n",
    "        return [fx,rmse]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BayesNN:  # Multi-Task leaning using Stocastic GD\n",
    "\n",
    "    def __init__(self, mtaskNet, traindata, testdata, samples, minPerf,   num_subtasks):\n",
    "        # trainData and testData could also be different datasets. this example considers one dataset\n",
    "\n",
    "        self.traindata = traindata\n",
    "        self.testdata = testdata\n",
    "        self.samples = samples\n",
    "        self.minCriteria = minPerf\n",
    "        self.subtasks = num_subtasks  # number of network modules\n",
    "        # need to define network toplogies for the different tasks.\n",
    "\n",
    "        self.mtaskNet = mtaskNet\n",
    "\n",
    "        self.trainTolerance = 0.20  # [eg 0.15 output would be seen as 0] [ 0.81 would be seen as 1]\n",
    "        self.testTolerance = 0.49\n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "    def net_size(self, netw):\n",
    "        return ((netw[0] * netw[1]) + (netw[1] * netw[2]) + netw[1] ) #+ netw[2]\n",
    "\n",
    "    def likelihood_func(self, neuralnet,  y,  w, tausq, subtasks):\n",
    "        #print y, ' ..y'\n",
    "\n",
    "        fx = neuralnet.evaluate_proposal(w, self.mtaskNet, subtasks)\n",
    "        #print fx, ' fx'\n",
    "        #print y, ' y'\n",
    "        rmse = self.rmse(fx, y)\n",
    "        #print rmse, '  .. rmse  '\n",
    "        #print fx[0:5],  ' ...fx'\n",
    "        #print y[0:5], ' ... y'\n",
    "        loss = -0.5 * np.log(2 * math.pi * tausq) - 0.5 * np.square(y - fx) / tausq\n",
    "        return [np.sum(loss), fx, rmse]\n",
    "\n",
    "\n",
    "\n",
    "    def prior_likelihood(self, sigma_squared, nu_1, nu_2, w, tausq, topo):\n",
    "        h = topo[1]  # number hidden neurons\n",
    "        d = topo[0]  # number input neurons\n",
    "        part1 = -1 * ((d * h + h + 2) / 2) * np.log(sigma_squared)\n",
    "        part2 = 1 / (2 * sigma_squared) * (sum(np.square(w)))\n",
    "        log_loss = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "        return log_loss\n",
    "\n",
    "    def taskdata(self, data, taskfeatures, output):\n",
    "        # group taskdata from main data source.\n",
    "        # note that the grouping is done in accending order fin terms of features.\n",
    "        # the way the data is grouped as tasks can change for different applications.\n",
    "        # there is some motivation to keep the features with highest contribution as first  feature space for module 1.\n",
    "        datacols = data.shape[1]\n",
    "        featuregroup = data[:, 0:taskfeatures]\n",
    "        return np.concatenate((featuregroup[:, range(0, taskfeatures)], data[:, range(datacols - output, datacols)]),\n",
    "                              axis=1)\n",
    "\n",
    "\n",
    "    def mcmc_sampler(self):\n",
    "\n",
    "    # ------------------- initialize MCMC\n",
    "        testsize = self.testdata.shape[0]\n",
    "        trainsize = self.traindata.shape[0]\n",
    "        samples = self.samples\n",
    "\n",
    "\n",
    "        x_test = np.linspace(0, 1, num=testsize)\n",
    "        x_train = np.linspace(0, 1, num=trainsize)\n",
    "\n",
    "\n",
    "        Netlist = [None] * 10  # create list of Network objects ( just max size of 10 for now )\n",
    "\n",
    "        samplelist = [None] * samples  # create list of Network objects ( just max size of 10 for now )\n",
    "\n",
    "        rmsetrain = np.zeros(self.subtasks)\n",
    "        rmsetest  = np.zeros(self.subtasks)\n",
    "        trainfx = np.random.randn(self.subtasks, trainsize)\n",
    "        testfx = np.random.randn(self.subtasks, testsize)\n",
    "\n",
    "        netsize = np.zeros(self.subtasks, dtype=np.int)\n",
    "\n",
    "\n",
    "        depthSearch = 5  # declare\n",
    "\n",
    "\n",
    "        for n in xrange(0, self.subtasks):\n",
    "            module = self.mtaskNet[n]\n",
    "            trdata = self.taskdata(self.traindata, module[0], module[2])  # make the partitions for task data\n",
    "            testdata = self.taskdata(self.testdata, module[0], module[2])\n",
    "            Netlist[n] = Network(self.mtaskNet[n], trdata, testdata, depthSearch, self.minCriteria)\n",
    "            #print trdata\n",
    "\n",
    "\n",
    "\n",
    "        #trdata = self.taskdata(self.traindata, module[0], module[2])  # make the partitions for task data\n",
    "        #testdata = self.taskdata(self.testdata, module[0], module[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for n in xrange(0, self.subtasks):\n",
    "            netw = Netlist[n].Top\n",
    "            netsize[n] =  self.net_size(netw)  # num of weights and bias\n",
    "            print netsize[n]\n",
    "\n",
    "\n",
    "        y_test = testdata[:, netw[0]]  #grab the actual predictions from dataset\n",
    "        y_train = trdata[:, netw[0]]\n",
    "\n",
    "        w_pos = np.zeros((samples, self.subtasks, netsize[self.subtasks-1]))  # 3D np array\n",
    "\n",
    "        posfx_train = np.zeros((samples, self.subtasks, trainsize))\n",
    "        posfx_test = np.zeros((samples, self.subtasks, testsize))\n",
    "\n",
    "        posrmse_train = np.zeros((samples, self.subtasks))\n",
    "        posrmse_test = np.zeros((samples, self.subtasks))\n",
    "\n",
    "        pos_tau = np.zeros(samples)\n",
    "\n",
    "        print posrmse_test\n",
    "        print posfx_test\n",
    "        print pos_tau, ' pos_tau'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        w = np.random.randn( self.subtasks, netsize[self.subtasks-1])\n",
    "\n",
    "        w_pro =  np.random.randn( self.subtasks, netsize[self.subtasks-1]) #parameters of subtasks constructed based on that of subtask m and m-1\n",
    "\n",
    "\n",
    "\n",
    "        step_w = 0.05;  # defines how much variation you need in changes to w\n",
    "        step_eta = 0.01\n",
    "\n",
    "        print 'evaluate Initial w'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pred_train = Netlist[0].evaluate_proposal(w[0,:netsize[0]], self.mtaskNet, 0)  # we only take prior calculation for first ensemble, since we have one tau value for all the ensembles.\n",
    "\n",
    "        eta = np.log(np.var(pred_train - y_train))\n",
    "        tau_pro = np.exp(eta)\n",
    "\n",
    "        sigma_squared = 25\n",
    "        nu_1 = 0\n",
    "        nu_2 = 0\n",
    "\n",
    "\n",
    "\n",
    "        likelihood = np.zeros(self.subtasks)\n",
    "        likelihood_pro = np.zeros(self.subtasks)\n",
    "\n",
    "        prior_likelihood = np.zeros(self.subtasks)\n",
    "\n",
    "        prior_pro = np.zeros(self.subtasks)\n",
    "\n",
    "        for n in xrange(0, self.subtasks):\n",
    "            prior_likelihood[n] = self.prior_likelihood(sigma_squared, nu_1, nu_2, w[n,:netsize[0]], tau_pro,  Netlist[n].Top)  # takes care of the gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Ok, in this case, we only consider the number of hidden neurons increasing for each subtask. Input neurons and output neuron remain fixed in this version.\n",
    "\n",
    "        mh_prob = np.zeros(self.subtasks)\n",
    "\n",
    "        for s in xrange(0, self.subtasks-1): #initialize for each subtask\n",
    "           [likelihood[s],  posfx_train[0, s,:],  posrmse_train[0, s]] = self.likelihood_func(Netlist[s], y_train, w[s,:netsize[s]], tau_pro, s)\n",
    "           w[s + 1, :netsize[s]] = w[s, :netsize[s]]\n",
    "\n",
    "        s = self.subtasks - 1\n",
    "        [likelihood[s], posfx_train[0, s, :], posrmse_train[0, s]] = self.likelihood_func(Netlist[s], y_train,  w[s, :netsize[s]], tau_pro, s)\n",
    "\n",
    "\n",
    "        naccept = 0\n",
    "\n",
    "\n",
    "\n",
    "        for i in xrange(1, samples-1):    # ---------------------------------------\n",
    "\n",
    "            for s in xrange(0, self.subtasks):\n",
    "                w_pro[s, :netsize[s]] = w[s, :netsize[s]] + np.random.normal(0, step_w, netsize[s]) #propose new set of weights given the subtask m\n",
    "\n",
    "\n",
    "\n",
    "            eta_pro  = eta  + np.random.normal(0, step_eta, 1)\n",
    "            tau_pro = math.exp(eta_pro)\n",
    "\n",
    "            for s in xrange(0, self.subtasks-1):\n",
    "                [likelihood_pro[s],  trainfx[s, :], rmsetrain[s]] = self.likelihood_func(Netlist[s], y_train, w_pro[s, :netsize[s]], tau_pro,s)\n",
    "                [testfx[s, :], rmsetest[s]] = Netlist[s].test_proposal(w_pro[s, :netsize[s]], self.mtaskNet, s)\n",
    "\n",
    "                w_pro[s + 1, :netsize[s]] = w_pro[s, :netsize[s]]\n",
    "\n",
    "            s = self.subtasks  -1\n",
    "\n",
    "            [likelihood_pro[s], trainfx[s, :], rmsetrain[s]] = self.likelihood_func(Netlist[s], y_train,  w_pro[s, :netsize[s]],  tau_pro, s)\n",
    "            [testfx[s, :],rmsetest[s]] = Netlist[s].test_proposal(w_pro[s,:netsize[s]], self.mtaskNet, s)\n",
    "\n",
    "\n",
    "\n",
    "            for n in xrange(0, self.subtasks):\n",
    "                prior_pro[n] = self.prior_likelihood(sigma_squared, nu_1, nu_2, w_pro[s, :netsize[s]], tau_pro, Netlist[n].Top)\n",
    "\n",
    "\n",
    "            diff = likelihood_pro  - likelihood\n",
    "            diff_prior = prior_pro - prior_likelihood\n",
    "\n",
    "\n",
    "            for s in xrange(0, self.subtasks):\n",
    "\n",
    "                mh_prob[s] = min(1, math.exp(diff[s] + diff_prior[s])) #acceptance rate\n",
    "\n",
    "                u = random.uniform(0, 1) #draw from uniform distribution\n",
    "\n",
    "\n",
    "                if u < mh_prob[s]:\n",
    "                    naccept += 1\n",
    "                    print i, ' is accepted sample'\n",
    "                    likelihood[s] = likelihood_pro[s]\n",
    "                    w[s,:netsize[s]] = w_pro[s,:netsize[s]]  # _w_proposal\n",
    "                    #x_ = x_pro\n",
    "\n",
    "\n",
    "\n",
    "                    eta = eta_pro\n",
    "\n",
    "                    prior_likelihood[s] = prior_pro[s]\n",
    "\n",
    "                    #print rmsetrain[s]\n",
    "\n",
    "                    print likelihood_pro[s], prior_pro[s], rmsetrain[s], rmsetest[s], s,  '   for s'  # takes care of the gradients\n",
    "\n",
    "                    print likelihood_pro, prior_pro, rmsetrain, rmsetest, '   for all'  # takes care of the gradients\n",
    "\n",
    "                    w_pos[i+1, s, :netsize[s]] = w_pro[s, :netsize[s]]\n",
    "\n",
    "\n",
    "                    posfx_train[i+1, s, :] = trainfx[s, :]\n",
    "                    posfx_test[i+1, s, :] = testfx[s, :]\n",
    "\n",
    "                    posrmse_train[i+1,s] = rmsetrain[s]\n",
    "                    posrmse_test[i+1,s] = rmsetest[s]\n",
    "\n",
    "                    pos_tau[i+1] = tau_pro\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    w_pos[i + 1, s, :netsize[s]] = w_pos[i, s, :netsize[s]]\n",
    "\n",
    "                    posfx_train[i+1, s, :] = posfx_train[i, s, :]\n",
    "                    posfx_test[i+1, s, :] = posfx_test[i, s, :]\n",
    "\n",
    "                    posrmse_train[i+1,s] = posrmse_train[i,s]\n",
    "                    posrmse_test[i+1,s] = posrmse_test[i,s]\n",
    "\n",
    "                    pos_tau[i + 1] =  pos_tau[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print naccept, ' num accepted'\n",
    "        accept_ratio = naccept / (samples * self.subtasks *1.0) * 100\n",
    "\n",
    "\n",
    "        return (w_pos, posfx_train, posfx_test, posrmse_train, posrmse_test, pos_tau, x_train, x_test,  y_test, y_train, accept_ratio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "\n",
    "    moduledecomp = [3, 5, 7]  # decide what will be number of features for each group of taskdata correpond to module\n",
    "    #the subtasks are created by windows of size [3, 5, 7]\n",
    "    for problem in range(1, 8):\n",
    "\n",
    "        path = str(problem) + 'res'\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            if not os.path.isdir(path):\n",
    "                raise\n",
    "        outres = open(path + '/results.txt', 'w')\n",
    "\n",
    "        hidden = 5\n",
    "        input = 7  # max input\n",
    "        output = 1\n",
    "        num_samples = 8000 # 80 000 used in exp. note total num_samples is num_samples * num_subtasks\n",
    "\n",
    "        if problem == 1:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Lazer/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Lazer/test7.txt\")  #\n",
    "        if problem == 2:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Sunspot/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Sunspot/test7.txt\")  #\n",
    "        if problem == 3:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Mackey/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Mackey/test7.txt\")  #\n",
    "        if problem == 4:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Lorenz/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Lorenz/test7.txt\")  #\n",
    "        if problem == 5:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Rossler/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Rossler/test7.txt\")  #\n",
    "        if problem == 6:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/Henon/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/Henon/test7.txt\")  #\n",
    "        if problem == 7:\n",
    "            traindata = np.loadtxt(\"Data_OneStepAhead/ACFinance/train7.txt\")\n",
    "            testdata = np.loadtxt(\"Data_OneStepAhead/ACFinance/test7.txt\")  #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        min_perf = 0.0000001  # stop when RMSE reches this point\n",
    "\n",
    "        subtasks = 3 #\n",
    "\n",
    "        baseNet = [input, hidden, output]\n",
    "\n",
    "\n",
    "        mtaskNet = np.array([baseNet, baseNet, baseNet])\n",
    "\n",
    "        for i in xrange(1, subtasks):\n",
    "            mtaskNet[i - 1][0] = moduledecomp[i - 1]\n",
    "            mtaskNet[i][1] += (i * 2)  # in this example, we have fixed numner  output neurons. input for each task is determined by feature group size.\n",
    "            # we adapt the number of hidden neurons for each task.\n",
    "        print mtaskNet  # print network topology of all the modules that make the respective tasks. Note in this example, the tasks aredifferent network topologies given by hiddent number of hidden layers.\n",
    "\n",
    "\n",
    "\n",
    "        bayesnn = BayesNN(mtaskNet, traindata, testdata, num_samples, min_perf,   subtasks)\n",
    "\n",
    "\n",
    "\n",
    "        [w_pos, posfx_train, posfx_test, posrmse_train, posrmse_test, pos_tau, x_train, x_test, y_test, y_train, accept_ratio] = bayesnn.mcmc_sampler()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print 'sucessfully sampled'\n",
    "\n",
    "        burnin = 0.1 * num_samples  # use post burn in samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        rmsetr = np.zeros(subtasks)\n",
    "        rmsetr_std = np.zeros(subtasks)\n",
    "        rmsetes = np.zeros(subtasks)\n",
    "        rmsetes_std = np.zeros(subtasks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print accept_ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for s in xrange(0, subtasks):\n",
    "            rmsetr[s] = scipy.mean(posrmse_train[int(burnin):,s])\n",
    "            rmsetr_std[s] = np.std(posrmse_train[int(burnin):,s])\n",
    "            rmsetes[s]= scipy.mean(posrmse_test[int(burnin):,s])\n",
    "            rmsetes_std[s] = np.std(posrmse_test[int(burnin):,s])\n",
    "\n",
    "\n",
    "\n",
    "        print rmsetr, rmsetr_std, rmsetes, rmsetes_std, 'subtask 1'\n",
    "\n",
    "\n",
    "\n",
    "        np.savetxt(outres, (problem, accept_ratio), fmt='%1.1f')\n",
    "        np.savetxt(outres, (rmsetr, rmsetr_std, rmsetes, rmsetes_std), fmt='%1.5f')\n",
    "\n",
    "\n",
    "\n",
    "        #next outputs  ------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fx_mu = posfx_test[int(burnin):,0,:].mean(axis=0)\n",
    "        fx_high = np.percentile(posfx_test[int(burnin):,0,:], 95, axis=0)\n",
    "        fx_low = np.percentile(posfx_test[int(burnin):,0,:], 5, axis=0)\n",
    "\n",
    "        fx_mu_tr = posfx_train[int(burnin):,0,:].mean(axis=0)\n",
    "        fx_high_tr = np.percentile(posfx_train[int(burnin):,0,:], 95, axis=0)\n",
    "        fx_low_tr = np.percentile(posfx_train[int(burnin):,0,:], 5, axis=0)\n",
    "\n",
    "\n",
    "        fx_mu1 = posfx_test[int(burnin):,1,:].mean(axis=0)\n",
    "        fx_high1 = np.percentile(posfx_test[int(burnin):,1,:], 95, axis=0)\n",
    "        fx_low1 = np.percentile(posfx_test[int(burnin):,1,:], 5, axis=0)\n",
    "\n",
    "        fx_mu_tr1 = posfx_train[int(burnin):,1,:].mean(axis=0)\n",
    "        fx_high_tr1 = np.percentile(posfx_train[int(burnin):,1,:], 95, axis=0)\n",
    "        fx_low_tr1 = np.percentile(posfx_train[int(burnin):,1,:], 5, axis=0)\n",
    "\n",
    "        fx_mu2 = posfx_test[int(burnin):, 2, :].mean(axis=0)\n",
    "        fx_high2= np.percentile(posfx_test[int(burnin):, 2, :], 95, axis=0)\n",
    "        fx_low2 = np.percentile(posfx_test[int(burnin):, 2, :], 5, axis=0)\n",
    "\n",
    "        fx_mu_tr2 = posfx_train[int(burnin):, 2, :].mean(axis=0)\n",
    "        fx_high_tr2 = np.percentile(posfx_train[int(burnin):, 2, :], 95, axis=0)\n",
    "        fx_low_tr2 = np.percentile(posfx_train[int(burnin):, 2, :], 5, axis=0)\n",
    "\n",
    "\n",
    "        x2 = fx_high_tr2 - fx_low_tr2\n",
    "\n",
    "        print x2, '  diff x2'\n",
    "\n",
    "\n",
    "        x1 = fx_high_tr1 - fx_low_tr1\n",
    "\n",
    "        print x1, '  diff x1'\n",
    "\n",
    "\n",
    "\n",
    "        raw_input()\n",
    "\n",
    "        #subtask 1\n",
    "\n",
    "        plt.plot(x_test, y_test, label='actual')\n",
    "        plt.plot(x_test, fx_mu, label='pred. (mean)')\n",
    "        plt.plot(x_test, fx_low, label='pred.(5th percen.)')\n",
    "        plt.plot(x_test, fx_high, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_test, fx_low, fx_high, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Test data prediction performance and uncertainity\")\n",
    "        plt.savefig(path + '/restest.png')\n",
    "        plt.savefig(path+ '/restest.svg', format='svg', dpi=600)\n",
    "        plt.clf()\n",
    "        # -----------------------------------------\n",
    "        plt.plot(x_train, y_train, label='actual')\n",
    "        plt.plot(x_train, fx_mu_tr, label='pred. (mean)')\n",
    "        plt.plot(x_train, fx_low_tr, label='pred.(5th percen.)')\n",
    "        plt.plot(x_train, fx_high_tr, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_train, fx_low_tr, fx_high_tr, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Train data prediction performance and uncertainity \")\n",
    "        plt.savefig(path + '/restrain.png')\n",
    "        plt.savefig( path + '/restrain.svg', format='svg', dpi=600)\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        #subtask 2\n",
    "\n",
    "\n",
    "        plt.plot(x_test, y_test, label='actual')\n",
    "        plt.plot(x_test, fx_mu1, label='pred. (mean)')\n",
    "        plt.plot(x_test, fx_low1, label='pred.(5th percen.)')\n",
    "        plt.plot(x_test, fx_high1, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_test, fx_low1, fx_high1, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Test data prediction performance and uncertainity\")\n",
    "        plt.savefig(path + '/restest1.png')\n",
    "        plt.savefig(path+ '/restest1.svg', format='svg', dpi=600)\n",
    "        plt.clf()\n",
    "        # ------------------------------1-----------\n",
    "        plt.plot(x_train, y_train, label='actual')\n",
    "        plt.plot(x_train, fx_mu_tr1, label='pred. (mean)')\n",
    "        plt.plot(x_train, fx_low_tr1, label='pred.(5th percen.)')\n",
    "        plt.plot(x_train, fx_high_tr1, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_train, fx_low_tr1, fx_high_tr1, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Train data prediction performance and uncertainity \")\n",
    "        plt.savefig(path + '/restrain1.png')\n",
    "        plt.savefig( path + '/restrain1.svg', format='svg', dpi=600)\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "        # subtask 3\n",
    "\n",
    "\n",
    "        plt.plot(x_test, y_test, label='actual')\n",
    "        plt.plot(x_test, fx_mu2, label='pred. (mean)')\n",
    "        plt.plot(x_test, fx_low2, label='pred.(5th percen.)')\n",
    "        plt.plot(x_test, fx_high2, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_test, fx_low2, fx_high2, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Test data prediction performance and uncertainity\")\n",
    "        plt.savefig(path + '/restest2.png')\n",
    "        plt.savefig(path+ '/restest2.svg', format='svg', dpi=600)\n",
    "        plt.clf()\n",
    "        # ------------------------------1-----------\n",
    "        plt.plot(x_train, y_train, label='actual')\n",
    "        plt.plot(x_train, fx_mu_tr2, label='pred. (mean)')\n",
    "        plt.plot(x_train, fx_low_tr2, label='pred.(5th percen.)')\n",
    "        plt.plot(x_train, fx_high_tr2, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_train, fx_low_tr2, fx_high_tr2, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.title(\"Train data prediction performance and uncertainity \")\n",
    "        plt.savefig(path + '/restrain2.png')\n",
    "        plt.savefig( path + '/restrain2.svg', format='svg', dpi=600)\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": main()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('env_pytorch': conda)"
  },
  "interpreter": {
   "hash": "164dfdaf806f60fca5f093b6a424355790f856c9afd16dfa99ca3711d938d3e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}