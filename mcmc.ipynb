{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Network:\n",
    "\tdef __init__(self, Topo, Train, Test, learn_rate):\n",
    "\t\tself.Top = Topo #NN structure [input, hidden, output]\n",
    "\t\tself.TrainData = Train\n",
    "\t\tself.TestData = Test\n",
    "\t\tnp.random.seed()\n",
    "\t\tself.lrate = learn_rate\n",
    "\n",
    "\t\tself.W1 = np.random.radn(self.Top[0], self.Top[1]/np.sqrt(self.Top[0]))\n",
    "\t\tself.B1 = np.random.radn(1, self.Top[1]/np.sqrt(self.Top[1])) #bias first layer\n",
    "\t\tself.W2 = np.random.radn(self.Top[1], self.Top[2]/np.sqrt(self.Top[1]))\n",
    "\t\tself.B2 = np.random.radn(1, self.Top[2]/np.sqrt(self.Top[1])) #bias second layer\n",
    "\n",
    "\t\tself.hidout = np.zeros((1, self.Top[1])) #output of hidden layer\n",
    "\t\tself.out = np.zeros((1,self.Top[2])) #output last layer\n",
    "\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn 1/(1 + np.exp(-x))\n",
    "\n",
    "\tdef sampleEr(self, actualout):\n",
    "\t\terror = np.subtract(self.out, actualout)\n",
    "\t\tsqerror = np.sum(np.square(error))/self.Top[2]\n",
    "\t\treturn sqerror\n",
    "\n",
    "\tdef ForwardPass(self, X):\n",
    "\t\tz1 = X.dot(self.W1) + self.B1\n",
    "\t\tself.hidout = self.sigmoid(z1) #output of first hidden layer\n",
    "\t\tz2 = self.hidout.dot(self.W2) + self.B2\n",
    "\t\tself.out = self.sigmoid(z2) #output of second layer\n",
    "\n",
    "\t#Backpropagation will give us a procedure to compute the error δj in the jth neuron in the lth layer, and then will relate error to ∂C/∂w (weight between jth and kth neuron in lth layer) and ∂C/∂bj\n",
    "\t#The error in the neuron changes the weighted input; the change propagates through later layers and change the cost \n",
    "\n",
    "\tdef BackPass(self, Input, desired):\n",
    "\t\tout_error = (desired - self.out)*(self.out*(1-self.out)) #derivation of sigmoid function\n",
    "\t\thid_error = out_error.dot(self.W2.T)*(self.hidout*(1-self.hidout))\n",
    "\n",
    "\t\t#self.W2 += self.hidout.T.dot(out_error) * self.lrate\n",
    "\t\t#self.B2 += out_error*self.lrate*-1\n",
    "\t\t#self.W1 += Input.T.dot(hid_error) * self.lrate\n",
    "\t\t#self.B2 += hid_error*self.lrate*-1\n",
    "\n",
    "\t\tlayer = 1 #hidden to output layer\n",
    "\t\tfor x in range(0, self.Top[layer]):\n",
    "\t\t\tfor y in range(0, self.Top[layer+1]):\n",
    "\t\t\t\tself.W2 += self.hidout[x]*out_error[y]*self.lrate\n",
    "\n",
    "\t\tfor y in range(0, self.Top[layer+1]):\n",
    "\t\t\tself.B2 += -1*out_error[y]*self.lrate\n",
    "\n",
    "\t\tlayer = 0\n",
    "\t\tfor x in range(0,self.Top[layer]):\n",
    "\t\t\tfor y in range(0, self.Top[layer+1]):\n",
    "\t\t\t\tself.W1 = Input[x]*hid_error[y]*self.lrate\n",
    "\n",
    "\t\tfor y in range(0, self.Top[layer+1]):\n",
    "\t\t\tself.B1 += -1*hid_error[y]*self.lrate\n",
    "\n",
    "\t#change the shape of numpy array\n",
    "\t#reshaping as first raveling the array (using the given index order), then inserting the elements from the raveled array into the new multidimensional array (aka tensor)\n",
    "\tdef decode(self, w):\n",
    "\t\tw_layer1size = self.Top[0] * self.Top[1]\n",
    "\t\tw_layer2size = self.Top[1] * self.Top[2]\n",
    "\n",
    "\t\tw_layer1 = w[0:w_layer1size]\n",
    "\t\tw_layer2 = w[w_layer1size:w_layer1size + w_layer2size]\n",
    "\n",
    "\t\tself.W1 = np.reshape(w_layer1, (self.Top[0],self.Top[1]))\n",
    "\t\tself.W2 = np.reshape(w_layer2, (self.Top[1],self.Top[2]))\n",
    "\t\tself.B1 = w[w_layer1size + w_layer2size:w_layer1size + w_layer2size + self.Top[1]]\n",
    "\t\tself.B2 = w[w_layer1size + w_layer2size + self.Top[1]:w_layer1size + w_layer2size + self.Top[1] + self.Top[2]]\n",
    "\t#flattern into the 1D array\n",
    "\tdef encode(self):\n",
    "\t\tw1 = self.W1.ravel()\n",
    "\t\tw2 = self.W2.ravel()\n",
    "\t\tw = np.concatenate([w1,w2,self.B1,self.B2])\n",
    "\t\treturn w\n",
    "\n",
    "\t#Langevin applied to Bayesian NN; samples from posterior distribution by updating the chain. As the algorithm approaches local min, gradient -> 0 and chain produces samples surrounding maximum a posteriori mode to allow for posterior inference\n",
    "\tdef langevin_gradient(self, w, data, depth):\n",
    "\t\tself.decode(w) \n",
    "\t\tsize = data.shape[0]\n",
    "\n",
    "\t\tInput = np.zeros((1, self.Top[0]))\n",
    "\t\tDesired = np.zeros((1, self.Top[2]))\n",
    "\t\tfx = np.zeros(size)\n",
    "\n",
    "\t\tfor i in range(0, depth):\n",
    "\t\t\tfor i in range(0, size):\n",
    "\t\t\t\tpat = i\n",
    "\t\t\t\tInput = data[pat, 0:self.Top[0]]\n",
    "\t\t\t\tDesired = data[pat, self.Top[0]:]\n",
    "\t\t\t\tself.ForwardPass(Input)\n",
    "\t\t\t\tself.BackwardPass(Input, Desired)\n",
    "\n",
    "\t\tw_updated = self.encode()\n",
    "\n",
    "\t\treturn w_updated \n",
    "\n",
    "\tdef evaluate_proposal(self,data,w):\n",
    "\t\tself.decode(w)\n",
    "\t\tsize = data.shape[0]\n",
    "\n",
    "\t\tInput = np.zeros((1, self.Top[0]))\n",
    "\t\tfx = np.zeros(size)\n",
    "\n",
    "\t\tfor i in range(0, size): #to see what fx is produced by the current weight update\n",
    "\t\t\tInput = data[i, 0:self.Top[0]]\n",
    "\t\t\tself.ForwardPass(Input)\n",
    "\t\t\tfx[i] = self.out\n",
    "\n",
    "\t\treturn fx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MCMC:\n",
    "    def __init__(self, samples, topology, traindata, testdata,likelihood_prob, use_langevin_gradients, learn_rate):\n",
    "        self.samples = samples\n",
    "        self.topology = topology\n",
    "        self.traindata = traindata\n",
    "        self.testdata = testdata\n",
    "        self.use_langevin_gradients  =  use_langevin_gradients \n",
    "        self.likelihood_prob = likelihood_prob # likelihood prob\n",
    "        self.learn_rate = learn_rate\n",
    "\n",
    "    def rmse(self, targets, predictions):\n",
    "        pred_error = targets - predictions\n",
    "        squared_error = np.square(pred_error)\n",
    "        return np.mean(squared_error)\n",
    "\n",
    "    def likelihood_func(self, neuralnet, data, w, tausq):\n",
    "\t\ty = data[:, self.topology[0]]\n",
    "\t\tfx = neuralnet.evaluate_proposal(data, w)\n",
    "\t\trmse = self.rmse(fx, y)\n",
    "\t\tloss = -0.5 * np.log(2 * math.pi * tausq) - 0.5 * np.square(y - fx) / tausq\n",
    "\t\treturn [np.sum(loss), fx, rmse]\n",
    "\n",
    "\tdef prior_likelihood(self, sigma_squared, nu_1, nu_2, w, tausq):\n",
    "\t\th = self.topology[1]  # number hidden neurons\n",
    "\t\td = self.topology[0]  # number input neurons\n",
    "\t\tpart1 = -1 * ((d * h + h + 2) / 2) * np.log(sigma_squared)\n",
    "\t\tpart2 = 1 / (2 * sigma_squared) * (sum(np.square(w)))\n",
    "\t\tlog_loss = part1 - part2  - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "\t\treturn log_loss\n",
    "\n",
    "\tdef sampler(self, w_limit, tau_limit):\n",
    "\n",
    "\t\t# ------------------- initialize MCMC\n",
    "\t\ttestsize = self.testdata.shape[0]\n",
    "\t\ttrainsize = self.traindata.shape[0]\n",
    "\t\tsamples = self.samples\n",
    "\n",
    "\n",
    "\t\tself.sgd_depth = 1\n",
    "\n",
    "\t\tx_test = np.linspace(0, 1, num=testsize)\n",
    "\t\tx_train = np.linspace(0, 1, num=trainsize)\n",
    "\n",
    "\t\tnetw = self.topology  # [input, hidden, output]\n",
    "\t\ty_test = self.testdata[:, netw[0]]\n",
    "\t\ty_train = self.traindata[:, netw[0]]\n",
    "\t\tprint(y_train.size)\n",
    "\t\tprint(y_test.size)\n",
    "\n",
    "\t\tw_size = (netw[0] * netw[1]) + (netw[1] * netw[2]) + netw[1] + netw[2]  # num of weights and bias\n",
    "\n",
    "\t\tpos_w = np.ones((samples, w_size))  # posterior of all weights and bias over all samples - target distribution of interest\n",
    "\t\tpos_tau = np.ones((samples, 1))\n",
    "\n",
    "\t\tfxtrain_samples = np.ones((samples, trainsize))  # fx of train data over all samples\n",
    "\t\tfxtest_samples = np.ones((samples, testsize))  # fx of test data over all samples\n",
    "\t\trmse_train = np.zeros(samples)\n",
    "\t\trmse_test = np.zeros(samples)\n",
    "\n",
    "\t\tw = np.random.randn(w_size)\n",
    "\t\tw_proposal = np.random.randn(w_size)\n",
    "\n",
    "\t\t#step_w = 0.05;  # defines how much variation you need in changes to w\n",
    "\t\t#step_eta = 0.2; # exp 0\n",
    "\n",
    "\n",
    "\t\tstep_w = w_limit  # defines how much variation you need in changes to w\n",
    "\t\tstep_eta = tau_limit #exp 1\n",
    "\t\t# --------------------- Declare FNN and initialize\n",
    "\t\t \n",
    "\t\tneuralnet = Network(self.topology, self.traindata, self.testdata, self.learn_rate)\n",
    "\t\tprint('evaluate Initial w')\n",
    "\n",
    "\t\tpred_train = neuralnet.evaluate_proposal(self.traindata, w) #output of last layer\n",
    "\t\tpred_test = neuralnet.evaluate_proposal(self.testdata, w)\n",
    "\n",
    "\t\teta = np.log(np.var(pred_train - y_train))\n",
    "\t\ttau_pro = np.exp(eta)\n",
    "\n",
    "\t\tsigma_squared = 25\n",
    "\t\tnu_1 = 0\n",
    "\t\tnu_2 = 0\n",
    " \n",
    "\n",
    "\t\tsigma_diagmat = np.zeros((w_size, w_size))  # for Equation 9 in Ref [Chandra_ICONIP2017]\n",
    "\t\tnp.fill_diagonal(sigma_diagmat, step_w)\n",
    "\n",
    "\t\tdelta_likelihood = 0.5 # an arbitrary position\n",
    "\n",
    "\n",
    "\t\tprior_current = self.prior_likelihood(sigma_squared, nu_1, nu_2, w, tau_pro)  # takes care of the gradients\n",
    "\n",
    "\t\t[likelihood, pred_train, rmsetrain] = self.likelihood_func(neuralnet, self.traindata, w, tau_pro)\n",
    "\t\t[likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(neuralnet, self.testdata, w, tau_pro)\n",
    "\n",
    "\t\tprint(likelihood, ' Initial likelihood')\n",
    "\n",
    "\t\tnaccept = 0\n",
    "\n",
    "\t\tlangevin_count = 0\n",
    "\t\t \n",
    "\n",
    "\n",
    "\t\tfor i in range(samples - 1):\n",
    "\n",
    "\n",
    "\t\t\tlx = np.random.uniform(0,1,1)\n",
    "\n",
    "\t\t\tif (self.use_langevin_gradients is True) and (lx< self.l_prob):  \n",
    "\t\t\t\tw_gd = neuralnet.langevin_gradient(self.traindata, w.copy(), self.sgd_depth) # Eq 8 #initiate w\n",
    "\t\t\t\tw_proposal = np.random.normal(w_gd, step_w, w_size) # Eq 7 #w drawn from arbitray distri \n",
    "\t\t\t\tw_prop_gd = neuralnet.langevin_gradient(self.traindata, w_proposal.copy(), self.sgd_depth) #update w with the proposed \n",
    "\t\t\t\t#first = np.log(multivariate_normal.pdf(w , w_prop_gd , sigma_diagmat)) \n",
    "\t\t\t\t#second = np.log(multivariate_normal.pdf(w_proposal , w_gd , sigma_diagmat)) # this gives numerical instability - hence we give a simple implementation next that takes out log \n",
    "\n",
    "\t\t\t\twc_delta = (w- w_prop_gd) \n",
    "\t\t\t\twp_delta = (w_proposal - w_gd )\n",
    "\n",
    "\t\t\t\tsigma_sq = step_w\n",
    "\n",
    "\t\t\t\tfirst = -0.5 * np.sum(wc_delta  *  wc_delta  ) / sigma_sq  # this is wc_delta.T  *  wc_delta /sigma_sq\n",
    "\t\t\t\tsecond = -0.5 * np.sum(wp_delta * wp_delta ) / sigma_sq\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tdiff_prop =  first - second  \n",
    "\t\t\t\tlangevin_count = langevin_count + 1\n",
    "\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tdiff_prop = 0\n",
    "\t\t\t\tw_proposal = np.random.normal(w, step_w, w_size)\n",
    "\n",
    " \t\t\t\n",
    "\t\t\teta_pro = eta + np.random.normal(0, step_eta, 1)\n",
    "\t\t\ttau_pro = math.exp(eta_pro)\n",
    "\n",
    "\t\t\t[likelihood_proposal, pred_train, rmsetrain] = self.likelihood_func(neuralnet, self.traindata, w_proposal,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttau_pro)\n",
    "\t\t\t[likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(neuralnet, self.testdata, w_proposal,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttau_pro) \n",
    "\n",
    "\t\t\tprior_prop = self.prior_likelihood(sigma_squared, nu_1, nu_2, w_proposal,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   tau_pro)  # takes care of the gradients\n",
    "\n",
    "\n",
    "\t\t\tdiff_prior = prior_prop - prior_current\n",
    "\n",
    "\t\t\tdiff_likelihood = likelihood_proposal - likelihood\n",
    "\n",
    "\t\t\t#mh_prob = min(1, math.exp(diff_likelihood + diff_prior + diff_prop))\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tmh_prob = min(1, math.exp(diff_likelihood+diff_prior+ diff_prop))\n",
    "\n",
    "\t\t\texcept OverflowError as e:\n",
    "\t\t\t\tmh_prob = 1\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tu = random.uniform(0, 1)\n",
    "\n",
    "\t\t\tif u < mh_prob:\n",
    "\t\t\t\t# Update position \n",
    "\t\t\t\tnaccept += 1\n",
    "\t\t\t\tlikelihood = likelihood_proposal\n",
    "\t\t\t\tprior_current = prior_prop\n",
    "\t\t\t\tw = w_proposal\n",
    "\t\t\t\teta = eta_pro\n",
    "\t\t\t\tif i%50 ==0:\n",
    "\t\t\t\t\tprint(i,likelihood, prior_current, diff_prop, rmsetrain, rmsetest, w, 'accepted')\n",
    "\t\t\t\t \n",
    "\n",
    "\t\t\t\tpos_w[i + 1,] = w_proposal\n",
    "\t\t\t\tpos_tau[i + 1,] = tau_pro\n",
    "\t\t\t\tfxtrain_samples[i + 1,] = pred_train\n",
    "\t\t\t\tfxtest_samples[i + 1,] = pred_test\n",
    "\t\t\t\trmse_train[i + 1,] = rmsetrain\n",
    "\t\t\t\trmse_test[i + 1,] = rmsetest\n",
    "\n",
    "\t\t\t\tplt.plot(x_train, pred_train)\n",
    "\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tpos_w[i + 1,] = pos_w[i,]\n",
    "\t\t\t\tpos_tau[i + 1,] = pos_tau[i,]\n",
    "\t\t\t\tfxtrain_samples[i + 1,] = fxtrain_samples[i,]\n",
    "\t\t\t\tfxtest_samples[i + 1,] = fxtest_samples[i,]\n",
    "\t\t\t\trmse_train[i + 1,] = rmse_train[i,]\n",
    "\t\t\t\trmse_test[i + 1,] = rmse_test[i,]\n",
    " \n",
    "\n",
    "\t\tprint(naccept, ' num accepted')\n",
    "\t\tprint(naccept / (samples * 1.0), '% was accepted')\n",
    "\t\taccept_ratio = naccept / (samples * 1.0) * 100\n",
    "\n",
    "\t\tprint(langevin_count, ' langevin_count')\n",
    "\n",
    " \n",
    "\n",
    "\t\treturn (pos_w, pos_tau, fxtrain_samples, fxtest_samples, x_train, x_test, rmse_train, rmse_test, accept_ratio)\n",
    "\n",
    "\n",
    "def main():\n",
    "\tfor problem in range(1, 4): \n",
    "\n",
    "\t\thidden = 5\n",
    "\t\tinput = 4  #\n",
    "\t\toutput = 1\n",
    "  \n",
    "\t\tw_limit =  0.025 # step size for w\n",
    "\t\ttau_limit = 0.2 # step size for eta\n",
    "\n",
    "\t\tif problem == 1:\n",
    "\t\t\ttraindata = np.loadtxt(\"data/Lazer/train.txt\")\n",
    "\t\t\ttestdata = np.loadtxt(\"data/Lazer/test.txt\")  #\n",
    "\t\t\tname\t= \"Lazer\"\n",
    "\t\tif problem == 2:\n",
    "\t\t\ttraindata = np.loadtxt(\"data/Sunspot/train.txt\")\n",
    "\t\t\ttestdata = np.loadtxt(\"data/Sunspot/test.txt\")  #\n",
    "\t\t\tname\t= \"Sunspot\"\n",
    "\t\tif problem == 3:\n",
    "\t\t\ttraindata = np.loadtxt(\"data/Mackey/train.txt\")\n",
    "\t\t\ttestdata = np.loadtxt(\"data/Mackey/test.txt\")  #\n",
    "\t\t\tname\t= \"Mackey\"\n",
    "\t\t  \n",
    "\n",
    "\t\ttopology = [input, hidden, output]\n",
    "\t\trandom.seed(time.time())\n",
    "\t\tnumSamples = 1000  # need to decide yourself\n",
    "\t\tuse_langevin_gradients  = True\n",
    "\n",
    "\t\tl_prob = 0.5\n",
    "\t\tlearn_rate = 0.01\n",
    "\n",
    "\t\ttimer = time.time() \n",
    "\t\tmcmc = MCMC( use_langevin_gradients , l_prob,  learn_rate, numSamples, traindata, testdata, topology)  # declare class\n",
    "\n",
    "\t\t[pos_w, pos_tau, fx_train, fx_test, x_train, x_test, rmse_train, rmse_test, accept_ratio] = mcmc.sampler(w_limit, tau_limit)\n",
    "\t\tprint('sucessfully sampled')\n",
    "\n",
    "\t\tburnin = 0.5 * numSamples  # use post burn in samples\n",
    "\t\t\n",
    "\t\ttimer2 = time.time()\n",
    "\n",
    "\t\ttimetotal = (timer2 - timer) /60\n",
    "\t\tprint((timetotal), 'min taken')\n",
    "\n",
    "\t\tpos_w = pos_w[int(burnin):, ]\n",
    "\t\tpos_tau = pos_tau[int(burnin):, ]\n",
    "\n",
    "\t\tfx_mu = fx_test.mean(axis=0)\n",
    "\t\tfx_high = np.percentile(fx_test, 95, axis=0)\n",
    "\t\tfx_low = np.percentile(fx_test, 5, axis=0)\n",
    "\n",
    "\t\tfx_mu_tr = fx_train.mean(axis=0)\n",
    "\t\tfx_high_tr = np.percentile(fx_train, 95, axis=0)\n",
    "\t\tfx_low_tr = np.percentile(fx_train, 5, axis=0)\n",
    "\n",
    "\t\tpos_w_mean = pos_w.mean(axis=0) \n",
    "\n",
    "\t\trmse_tr = np.mean(rmse_train[int(burnin):])\n",
    "\t\trmsetr_std = np.std(rmse_train[int(burnin):])\n",
    "\t\trmse_tes = np.mean(rmse_test[int(burnin):])\n",
    "\t\trmsetest_std = np.std(rmse_test[int(burnin):])\n",
    "\t\tprint(rmse_tr, rmsetr_std, rmse_tes, rmsetest_std)\n",
    "\n",
    " \n",
    "\t\toutres_db = open('result.txt', \"a+\")\n",
    "\n",
    "\t\tnp.savetxt(outres_db, ( use_langevin_gradients ,    learn_rate, rmse_tr, rmsetr_std, rmse_tes, rmsetest_std, accept_ratio, timetotal), fmt='%1.5f')\n",
    "\n",
    "\n",
    "\t\tytestdata = testdata[:, input]\n",
    "\t\tytraindata = traindata[:, input]\n",
    "\n",
    "\t\tplt.plot(x_test, ytestdata, label='actual')\n",
    "\t\tplt.plot(x_test, fx_mu, label='pred. (mean)')\n",
    "\t\tplt.plot(x_test, fx_low, label='pred.(5th percen.)')\n",
    "\t\tplt.plot(x_test, fx_high, label='pred.(95th percen.)')\n",
    "\t\tplt.fill_between(x_test, fx_low, fx_high, facecolor='g', alpha=0.4)\n",
    "\t\tplt.legend(loc='upper right')\n",
    "\n",
    "\t\tplt.title(\"Prediction  Uncertainty \")\n",
    "\t\tplt.savefig('mcmcrestest.png') \n",
    "\t\tplt.clf()\n",
    "\t\t# -----------------------------------------\n",
    "\t\tplt.plot(x_train, ytraindata, label='actual')\n",
    "\t\tplt.plot(x_train, fx_mu_tr, label='pred. (mean)')\n",
    "\t\tplt.plot(x_train, fx_low_tr, label='pred.(5th percen.)')\n",
    "\t\tplt.plot(x_train, fx_high_tr, label='pred.(95th percen.)')\n",
    "\t\tplt.fill_between(x_train, fx_low_tr, fx_high_tr, facecolor='g', alpha=0.4)\n",
    "\t\tplt.legend(loc='upper right')\n",
    "\n",
    "\t\tplt.title(\"Prediction  Uncertainty\")\n",
    "\t\tplt.savefig('mcmcrestrain.png') \n",
    "\t\tplt.clf()\n",
    "\n",
    "\t\tmpl_fig = plt.figure()\n",
    "\t\tax = mpl_fig.add_subplot(111)\n",
    "\n",
    "\t\tax.boxplot(pos_w)\n",
    "\t\tax.set_xlabel('[W1] [B1] [W2] [B2]')\n",
    "\t\tax.set_ylabel('Posterior')\n",
    "\t\tplt.legend(loc='upper right')\n",
    "\t\tplt.title(\"Boxplot of Posterior W (weights and biases)\")\n",
    "\t\tplt.savefig('w_pos.png')\n",
    "\t\t \n",
    "\t\tplt.clf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": main()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "8275eefe48575bd15bc42cc89d3bb7434dff6aa1b45db96084188feab7936b70"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}